# ðŸ§  Retrieval-Augmented Generation (RAG) Pipeline Prototype  
**Summer 2025 Project by LuckyCompiler001**

This repository contains a functional prototype of a **Retrieval-Augmented Generation (RAG)** system, which combines vector-based document retrieval with large language model (LLM) response generation. Itâ€™s designed as a modular, end-to-end pipeline to process and query your own document corpus.

---

## ðŸ“ Project Structure

```
â”œâ”€â”€ config.py               # Configuration file (API keys, model settings)
â”œâ”€â”€ main.py                 # Entry point to run the RAG pipeline
â”œâ”€â”€ ingest/                 # Load and preprocess raw documents
â”œâ”€â”€ embed/                  # Generate and store vector embeddings
â”œâ”€â”€ retrieval/              # Similarity-based document retrieval logic
â”œâ”€â”€ llm_generation/         # Prompt construction and LLM response handling
â”œâ”€â”€ helper_utilities/       # Shared utilities (I/O, logging, formatting)
â”œâ”€â”€ data/raw/               # Input files (PDFs, text, etc.)
â”œâ”€â”€ outputs/                # Logs and generated results
â”œâ”€â”€ LICENSE                 # License information
â””â”€â”€ README.md               # Project documentation
```

---

## âš™ï¸ Setup
Make sure to install the required dependencies. Populate your `requirements.txt` with:
openai
faiss-cpu
python-dotenv
Then, install them using:
bash
pip install -r requirements.txt

Also, ensure your `.env` file includes necessary credentials such as your OpenAI API key.

## ðŸš€ Usage

Run the pipeline using the `main.py` entry point:
### 1. Ingest & Embed Documents

bash
python main.py

When prompted, choose:
```
> build
```

This will load, preprocess, and embed your source documents.

### 2. Query the System

bash
python main.py

Then choose:
```
> query
```
Enter your question when prompted. For example:
```
> What are the main ideas from file X?
```
---
## ðŸ“Œ Notes

- The current version is a prototype and may require adaptation for large-scale or production use.
- All embeddings are stored locally; cloud-based vector databases can be integrated as needed.
- The system is model-agnosticâ€”swap out the LLM or embedding model via `config.py`.
---
Feel free to extend, modify, or contribute to improve this RAG system. Enjoy exploring your documents with AI!
